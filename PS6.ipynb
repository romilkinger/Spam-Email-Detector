{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b647fe59",
   "metadata": {},
   "source": [
    "# Problem Set 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3b5c7",
   "metadata": {},
   "source": [
    "# Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d4ae8",
   "metadata": {},
   "source": [
    "## INFO 371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b208a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bd210",
   "metadata": {},
   "source": [
    "## 1. (7pt) Load and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86632c27",
   "metadata": {},
   "source": [
    "1. (1pt) Load and clean data. Feel free to copy-paste from your PS05 solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d770b5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>files</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg1.txt</td>\n",
       "      <td>Subject: re : 2 . 882 s - &gt; np np  &gt; date : su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg2.txt</td>\n",
       "      <td>Subject: s - &gt; np + np  the discussion of s - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>3-1msg3.txt</td>\n",
       "      <td>Subject: 2 . 882 s - &gt; np np  . . . for me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>3-375msg1.txt</td>\n",
       "      <td>Subject: gent conference  \" for the listserv \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>3-378msg1.txt</td>\n",
       "      <td>Subject: query : causatives in korean  could a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    spam          files                                            message\n",
       "0  False    3-1msg1.txt  Subject: re : 2 . 882 s - > np np  > date : su...\n",
       "1  False    3-1msg2.txt  Subject: s - > np + np  the discussion of s - ...\n",
       "2  False    3-1msg3.txt  Subject: 2 . 882 s - > np np  . . . for me it ...\n",
       "3  False  3-375msg1.txt  Subject: gent conference  \" for the listserv \"...\n",
       "4  False  3-378msg1.txt  Subject: query : causatives in korean  could a..."
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = pd.read_csv('/Users/romilkinger/Downloads/lingspam-emails.csv.bz2', sep='\\t')\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "5ca61c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data - ps5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7b070b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam       0\n",
       "files      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "49546a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 3)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35aac0",
   "metadata": {},
   "source": [
    "2. (2pt) Vectorize emails so you have a DTM (I’ll refer to this as the design matrix X) and the\n",
    "spam/non-spam indicator y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "61cedd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "# define vectorizer\n",
    "X = vectorizer.fit_transform(emails.message)\n",
    "# vectorize your data. Note: this creates a sparse matrix, # use .toarray() if you run into trouble\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# in case you want to see what are the actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "be2bcd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60925"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a07d9700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 60925)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f61faead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 3)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289302e8",
   "metadata": {},
   "source": [
    "3. How many different documents (emails) and different tokens (words) do you have in these data? (2pt) Split data into training/validation chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca562d",
   "metadata": {},
   "source": [
    "there are 2893 documents (emails) and 60925 tokens (words) in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "2e63e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = emails.spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d58dcbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40a534",
   "metadata": {},
   "source": [
    "4.\n",
    "(2pt) Design a scheme to name your variables so you can understand (and you grader can understand too!) which mathematical concept it refers to. You need variables like (see Lecture notes, Ch 7.3 for more examples/explanations):\n",
    "\n",
    "• Pr(S = 1): probability of spam\n",
    "\n",
    "• Pr(S = 0|W = 1): probability the email is not spam given it cointains the word W \n",
    "\n",
    "• log Pr(W = 1): log probability of word present\n",
    "\n",
    "• l(S = 1|W): log-likelihood of email being spam, given vector of words it contains.\n",
    "Explain how do you name these examples values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c3477",
   "metadata": {},
   "source": [
    "schema for variables: \n",
    "    \n",
    "    - Pr(s=1) = pr_s1\n",
    "    - Pr(s=0|w=1) = pr_s0w1\n",
    "    - log Pr(W=1) = lpr_w1\n",
    "    - l(S=1 | W) = l_s1w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265eaa23",
   "metadata": {},
   "source": [
    "I choose this schema because it is consistent with the mathematical notation and easily interpretable by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16662606",
   "metadata": {},
   "source": [
    "## 2. (42pt) Naïve Bayes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93567a",
   "metadata": {},
   "source": [
    "1. (4pt) Here is a small excerpt from the initial DTM (before you split it into training/validation), corresponding to rows 983 to 985, and to columns 40,042–40,046:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "14c4e30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[982:985, 40041:40046].toarray()\n",
    "# array([[0, 0, 0, 0, 0],\n",
    "#        [0, 1, 0, 1, 0],\n",
    "#        [0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9712505d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nooteboom', 'nootka', 'nope', 'nor', 'nora']"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[40041:40046]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d5e2",
   "metadata": {},
   "source": [
    "(a) which emails do the rows correspond to?\n",
    "\n",
    "the rows correspond to emails 982, 983, 984\n",
    "\n",
    "(b) Which words do the columns correspond to?\n",
    "\n",
    "the words correspond to ['nooteboom', 'nootka', 'nope', 'nor', 'nora']\n",
    "\n",
    "(c) What do the “1”-s in the middle of the table mean? \n",
    "\n",
    "1s in the middle of the table means that those words exists in those corresponding emails.\n",
    "\n",
    "(d) What do the zeros mean?\n",
    "\n",
    "zeros mean that those words don't exists in those corresponding emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df62c56",
   "metadata": {},
   "source": [
    "2. (2pt) What is the accuracy of the naive model that predicts all emails into the majority category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "e1547143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8337366055997235"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y == False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588088d",
   "metadata": {},
   "source": [
    "the accuracy of model that predicts all emails into the majority category is 16%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835cc9a",
   "metadata": {},
   "source": [
    "3. (3pt) Compute the unconditional (log) probability that the email is spam/non-spam, log Pr(S = 1), and log Pr(S = 0). These probabilities are based on the values of y (i.e. spam) alone. They do not contain information about the words in emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5ba4fd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.790895538288791"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_s1 = math.log((y_train == True).mean())\n",
    "lpr_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7c1fc0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1824944325831309"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_s0 = math.log((y_train == False).mean())\n",
    "lpr_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b15e60",
   "metadata": {},
   "source": [
    "4. (8pt) For each word w, compute the (log) probability that the word is present in spam emails, log Pr(W = 1|S = 1), and (log) probability that the word is present in non-spam emails, log Pr(W = 1|S = 0). These probabilities can easily be calculated from counts of how many times these words are present for each class.\n",
    "Hint: these computations are based on your BOW-s X. Look at ways to sum along columns in this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "43438e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first check the size to understand that there are 60925 words \n",
    "#therefore the result of pr_w1s1 should also have this many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "dde5af5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 60925)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "05df69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of 2893 rows of email data, get all the spam indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "81c2a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_index = np.where(y_train == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "39b7720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_index[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c385899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know there are 393 emails in the training set of 2314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "39ca72d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_scam = (y_train == True).sum()\n",
    "total_scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "3b96fef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_no_scam = (y_train == False).sum()\n",
    "total_no_scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "e5abd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important to remember that we have to calculate log pr(w = 1 | s=1) \n",
    "# so first we calculated all the number of spam emails in the training set #totol_scam\n",
    "# second we create an array of spam emails only to calculate the occurence of each word in spam emails\n",
    "# third we divide the occurence of each word (60925 words) by total spams emails to get probability\n",
    "# fourth we apply log on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "c6fc8749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# puts all the spam emails in a list\n",
    "spam_emails = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (i in spam_index[0]):\n",
    "        spam_emails.append(X_train[i,:].toarray().flatten())\n",
    "    if (i % 1000 == 0):\n",
    "        print (i/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7e302a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns that list into an array for easier calculations on columns\n",
    "spam_emails_arr = np.array(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "459e3381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_emails) #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "7dffc777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 1, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 1, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails #list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "346b069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 60925)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_emails_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "ad2bfc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate the sum of each word in spam emails and put that sum in the list\n",
    "sum_w1s1 = []\n",
    "for i in range(spam_emails_arr.shape[1]):\n",
    "    sum_w1s1.append((spam_emails_arr[:,i]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "e1f72d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for testing purpose\n",
    "spam_emails_arr[:,0].sum() # sum of first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "1ac511a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60925"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sum_w1s1) #total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "537e5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the sum list into an array\n",
    "sum_w1s1_arr = np.array(sum_w1s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b9201135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60925,)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_w1s1_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "d86044fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide each word's sum by total scam emails to get probability\n",
    "pr_w1s1 = sum_w1s1_arr / total_scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "efd9237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60925,)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_w1s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0d60e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get their log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "fc3a1c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/d4bm7t3x453fztx7pd3j6w1r0000gn/T/ipykernel_11126/2162149394.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  lpr_w1s1 = np.log(pr_w1s1)\n"
     ]
    }
   ],
   "source": [
    "lpr_w1s1 = np.log(pr_w1s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "477bed01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11955546, -1.1355558 , -5.95583737, ...,        -inf,\n",
       "       -5.95583737,        -inf])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_w1s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "b5297752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 0 in sum_w1s1 and by apply log on them, they become -inf\n",
    "# for likelihood we compare the a > b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "3f8cd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now repeat the same process for log Pr(w=1|s=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "81909443",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_spam_index = np.where(y_train == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "60188ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# puts all the non spam emails in a list\n",
    "not_spam_emails = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    if (i in non_spam_index[0]):\n",
    "        not_spam_emails.append(X_train[i,:].toarray().flatten())\n",
    "    if (i % 1000 == 0):\n",
    "        print (i/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "196d4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns that list into an array for easier calculations on columns\n",
    "not_spam_emails_arr = np.array(not_spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "590bf703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1928, 60925)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_spam_emails_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9ac554de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caculate the sum of each word in non spam emails and put that sum in the list\n",
    "sum_w1s0 = []\n",
    "for i in range(not_spam_emails_arr.shape[1]):\n",
    "    sum_w1s0.append((not_spam_emails_arr[:,i]).sum())\n",
    "sum_w1s0_arr = np.array(sum_w1s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "5f667475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide each word's sum by total non scam emails to get probability\n",
    "pr_w1s0 = sum_w1s0_arr / total_no_scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "958cd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/d4bm7t3x453fztx7pd3j6w1r0000gn/T/ipykernel_11126/2915524138.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  lpr_w1s0 = np.log(pr_w1s0)\n"
     ]
    }
   ],
   "source": [
    "lpr_w1s0 = np.log(pr_w1s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "5d7176c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.84065337, -3.0424499 , -7.56423848, ..., -7.56423848,\n",
       "              -inf,        -inf])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_w1s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bad926",
   "metadata": {},
   "source": [
    "5. (1pt) What should be the dimension of your log Pr(W = 1|S = 0) and log Pr(W = 1|S = 1) vectors? Explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ea36c",
   "metadata": {},
   "source": [
    "the dimensions of log Pr(w=1|s=0) and log Pr(w=1|s=1) should be 60925 because that's the total number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f1fb9",
   "metadata": {},
   "source": [
    "Now we are done with the estimator. Your fitted model is completely described by these four probability vectors: logPr(S = 1),logPr(S = 1),logPr(W = 1|S = 1),logPr(W = 1|S = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "e019a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.790895538288791"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "6dc26831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1824944325831309"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "cc49830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11955546, -1.1355558 , -5.95583737, ...,        -inf,\n",
       "       -5.95583737,        -inf])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_w1s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "ca39a2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.84065337, -3.0424499 , -7.56423848, ..., -7.56423848,\n",
       "              -inf,        -inf])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpr_w1s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd008a7",
   "metadata": {},
   "source": [
    "Let’s now pull out your validation data and do some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb1af2",
   "metadata": {},
   "source": [
    "6. (10pt) For both classes, S = 1 and S = 0, compute the log-likelihood that the email belongs to this class. \n",
    "\n",
    "\n",
    "Log-likelihood is given as (7.3.20 and 7.3.21, page 270 for now) in lecture notes, and the equations in Schutt “Doing Data Science”, page 102.\n",
    "\n",
    "\n",
    "Computing the likelihoods involves sums of the previously computed probabilities, log Pr(W = 1|S), and BOW elements xij. Start by doing this by whatever way you can get it done (e.g. loops). The most important thing is that you understand what you do!\n",
    "\n",
    "\n",
    "But if you want to write efficient code, use matrix product instead (it is ∼ 1000× faster than loops). See Lecture Notes (7.30.30) for how to do it with matrix product. You can also check out np.apply_along_axis as an alternative way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "c54d3919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "3fffbcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579,)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_s1w = lpr_s1 + (lpr_w1s1 @ X_test.T)\n",
    "l_s1w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "4000bf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(579,)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_s0w = lpr_s0 + (lpr_w1s0 @ X_test.T)\n",
    "l_s0w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83276199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ee1c9da",
   "metadata": {},
   "source": [
    "7. (2pt) How many log-likelihoods you have to compute? Explain why do you have to have this many log-likelihoods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419f850",
   "metadata": {},
   "source": [
    "I had to compute two set of log-likelihood: log-likelihood of email being spam given the word and log-likelihood of email not being spam given the word. Both of which contained 579 emails as that was the testing/validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d448c95",
   "metadata": {},
   "source": [
    "8. (7pt) Based on the log-likelihoods, predict the class S = 1 or S = 0 for each email in the validation set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "e0de4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = (l_s1w > l_s0w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad59d",
   "metadata": {},
   "source": [
    "9. (3pt) Print the resulting confusion matrix and accuracy (feel free to use existing libraries).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "92649bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "156885c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[484,   0],\n",
       "       [ 85,  10]])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "2420bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.853195164075993\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ',accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8738b",
   "metadata": {},
   "source": [
    "10. (5pt) If your results are like mine, you can see that the results are not impressive at all, your model works no better than the naive guess. Explain why do you get such mediocre results.\n",
    "\n",
    "Hint: this is related to infinites, where are those coming from, and why they make the model useless? See also the smoothing-related discussion in Lecture Notes at the end of the Naive Bayes (Section 7.3.3), before Example 7.3.\n",
    "Note: just explain, but do not do anything about it! We’ll attack the problem in the next question with smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bf88e",
   "metadata": {},
   "source": [
    "Since there are some words that occur only in one or more class therefore the predictions based on those words would result in 100% as spam or not spam, which can be see by -inf in our prediction log likelihood. Therefore the accuracy is only as good as naive guess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173cd48",
   "metadata": {},
   "source": [
    "## 3. (32pt) Add smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf0c6f",
   "metadata": {},
   "source": [
    "So, now you have your brand-new NB algorithm up and running. But the results are not impressive... As a next step, we add smoothing to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c5d7a",
   "metadata": {},
   "source": [
    "1. (2pt) As you will be doing validation below, your first task is to mold what you did above into two functions: one for fitting and another one for predicting. You can mostly copy-paste your code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "eeeeff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not putting total spam / non-spam emails, spam/nonspam index\n",
    "def fitting():\n",
    "    # puts all the spam emails in a list\n",
    "    lpr_s0 = math.log((y_train == False).mean())\n",
    "    lpr_s1 = math.log((y_train == True).mean())\n",
    "    total_scam = (y_train == True).sum()\n",
    "    total_no_scam = (y_train == False).sum()\n",
    "    spam_index = np.where(y_train == True)\n",
    "    non_spam_index = np.where(y_train == False)\n",
    "    #finding lpr_w1s1 spam emails\n",
    "    spam_emails = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        if (i in spam_index[0]):\n",
    "            spam_emails.append(X_train[i,:].toarray().flatten())\n",
    "    spam_emails_arr = np.array(spam_emails)\n",
    "    sum_w1s1 = []\n",
    "    for i in range(spam_emails_arr.shape[1]):\n",
    "        sum_w1s1.append((spam_emails_arr[:,i]).sum())\n",
    "    sum_w1s1_arr = np.array(sum_w1s1)\n",
    "    pr_w1s1 = sum_w1s1_arr / total_scam \n",
    "    lpr_w1s1 = np.log(pr_w1s1)\n",
    "    #finding lpr_w1s0 non-spam emails\n",
    "    not_spam_emails = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        if (i in non_spam_index[0]):\n",
    "            not_spam_emails.append(X_train[i,:].toarray().flatten())\n",
    "    not_spam_emails_arr = np.array(not_spam_emails)\n",
    "    sum_w1s0 = []\n",
    "    for i in range(not_spam_emails_arr.shape[1]):\n",
    "        sum_w1s0.append((not_spam_emails_arr[:,i]).sum())\n",
    "    sum_w1s0_arr = np.array(sum_w1s0)\n",
    "    pr_w1s0 = sum_w1s0_arr / total_no_scam \n",
    "    lpr_w1s0 = np.log(pr_w1s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "13df358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    l_s1w = lpr_s1 + (lpr_w1s1 @ X_test.T)\n",
    "    l_s0w = lpr_s0 + (lpr_w1s0 @ X_test.T)\n",
    "    y_hat = (l_s1w > l_s0w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785d113",
   "metadata": {},
   "source": [
    "2. (18pt) Add smoothing to the model. Smoothing amounts to assuming that we have “seen” every possible word α ⩾ 0 times already, in both spam and non-spam emails. Note that α does not have to be an integer, and typically the best α < 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de92b5",
   "metadata": {},
   "source": [
    "What you have to do is to re-compute the probabilities logPr(S = 1),logPr(S = 0),logPr(W = 1|S = 1),logPr(W = 1|S = 1), the predictions part will remain unchanged. So you should up- date your fitting function by adding an additional argument α to it, and modify the probabilities accordingly. (And you use only training data for this.)\n",
    "See Lecture Notes 7.3.2; Example 7.3 (page 274); and Schutt p 103 and p 109 for more explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "89f00896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(smoothing):\n",
    "    smooth_lpr_s1 = np.log((len(y_train[y_train == True]) + smoothing) / len(y_train) + 2 * smoothing)\n",
    "    smooth_lpr_s0 = np.log((len(y_train[y_train == False]) + smoothing) / len(y_train) + 2 * smoothing)\n",
    "\n",
    "\n",
    "    smooth_lpr_w1s1 = np.log((np.sum(X_train[y_train==True], axis=0) + smoothing) / \n",
    "                     (np.sum(y_train == True) + 2 * smoothing))\n",
    "\n",
    "    smooth_lpr_w1s0 = np.log((np.sum(X_train[y_train==False], axis=0) + smoothing) / \n",
    "                     (np.sum(y_train == False) + 2 * smoothing))\n",
    "    return smooth_lpr_s1, smooth_lpr_s0, smooth_lpr_w1s1, smooth_lpr_w1s0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f97936",
   "metadata": {},
   "source": [
    "3. (5pt) Use your updated model for predictions with a few different α-values (on validation data) and report the corresponding confusion matrix and accuracy.\n",
    "A well-implemented algorith should not spend more than a few seconds on both fitting and predict- ing. However, more important that you understand what you are doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "f519bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    smooth_l_s1w = smooth_lpr_s1 + (smooth_lpr_w1s1 @ X_test.T)\n",
    "    smooth_l_s0w = smooth_lpr_s0 + (smooth_lpr_w1s0 @ X_test.T)\n",
    "    y_hat = (smooth_l_s1w > smooth_l_s0w)\n",
    "    y_hat, = np.array(y_hat)\n",
    "    con_matrix = confusion_matrix(y_test, y_hat) \n",
    "    return accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "f656c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9861830742659758\n"
     ]
    }
   ],
   "source": [
    "smooth_lpr_s1, smooth_lpr_s0, smooth_lpr_w1s1, smooth_lpr_w1s0 = fitting(0.1)\n",
    "print(prediction())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ec5de",
   "metadata": {},
   "source": [
    "4. \n",
    "(5pt) Use validation to find the best smoothing parameter α.\n",
    "You can just run a loop over different values, but start with very small values (10−8, 10−7, 10−6 up\n",
    "to perhaps 10).\n",
    "Note: this is fairly fast if your algorithm is fast. But even if your algorithm is slow, do the best you can!\n",
    "If your results are like mine, your best accuracy will be > 99.5%. (But this result is random!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "e2e6ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/d4bm7t3x453fztx7pd3j6w1r0000gn/T/ipykernel_11126/3722585712.py:6: RuntimeWarning: divide by zero encountered in log\n",
      "  smooth_lpr_w1s1 = np.log((np.sum(X_train[y_train==True], axis=0) + smoothing) /\n",
      "/var/folders/py/d4bm7t3x453fztx7pd3j6w1r0000gn/T/ipykernel_11126/3722585712.py:9: RuntimeWarning: divide by zero encountered in log\n",
      "  smooth_lpr_w1s0 = np.log((np.sum(X_train[y_train==False], axis=0) + smoothing) /\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.853195164075993,\n",
       " 0.9861830742659758,\n",
       " 0.9723661485319517,\n",
       " 0.9568221070811744,\n",
       " 0.9516407599309153,\n",
       " 0.9430051813471503,\n",
       " 0.9291882556131261,\n",
       " 0.9153713298791019,\n",
       " 0.9032815198618307,\n",
       " 0.8911917098445595,\n",
       " 0.8860103626943006,\n",
       " 0.8791018998272885,\n",
       " 0.8652849740932642,\n",
       " 0.8514680483592401,\n",
       " 0.842832469775475,\n",
       " 0.8307426597582038,\n",
       " 0.8186528497409327,\n",
       " 0.8031088082901554,\n",
       " 0.7927461139896373,\n",
       " 0.7772020725388601,\n",
       " 0.7599309153713298,\n",
       " 0.7495682210708118,\n",
       " 0.7374784110535406,\n",
       " 0.7236614853195165,\n",
       " 0.7167530224525043,\n",
       " 0.694300518134715,\n",
       " 0.6735751295336787,\n",
       " 0.6580310880829016,\n",
       " 0.6528497409326425,\n",
       " 0.6321243523316062,\n",
       " 0.6200345423143351,\n",
       " 0.6010362694300518,\n",
       " 0.5872193436960277,\n",
       " 0.5837651122625216,\n",
       " 0.5664939550949913,\n",
       " 0.5526770293609672,\n",
       " 0.5492227979274611,\n",
       " 0.538860103626943,\n",
       " 0.5250431778929189,\n",
       " 0.5112262521588946,\n",
       " 0.5060449050086355,\n",
       " 0.5025906735751295,\n",
       " 0.49222797927461137,\n",
       " 0.4905008635578584,\n",
       " 0.48013816925734026,\n",
       " 0.47322970639032813,\n",
       " 0.46632124352331605,\n",
       " 0.46286701208981,\n",
       " 0.4525043177892919,\n",
       " 0.44041450777202074,\n",
       " 0.4317789291882556,\n",
       " 0.4214162348877375,\n",
       " 0.4162348877374784,\n",
       " 0.40932642487046633,\n",
       " 0.4058721934369603,\n",
       " 0.39378238341968913,\n",
       " 0.38687392055267705,\n",
       " 0.38687392055267705,\n",
       " 0.38169257340241797,\n",
       " 0.3696027633851468,\n",
       " 0.3626943005181347,\n",
       " 0.3488773747841105,\n",
       " 0.34196891191709844,\n",
       " 0.34024179620034545,\n",
       " 0.3385146804835924,\n",
       " 0.3281519861830743,\n",
       " 0.3229706390328152,\n",
       " 0.31951640759930916,\n",
       " 0.3143350604490501,\n",
       " 0.30915371329879104,\n",
       " 0.30569948186528495,\n",
       " 0.2987910189982729,\n",
       " 0.29360967184801384,\n",
       " 0.29015544041450775,\n",
       " 0.2849740932642487,\n",
       " 0.28324697754749567,\n",
       " 0.27461139896373055,\n",
       " 0.2711571675302245,\n",
       " 0.26770293609671847,\n",
       " 0.26770293609671847,\n",
       " 0.26252158894645944,\n",
       " 0.2556131260794473,\n",
       " 0.24870466321243523,\n",
       " 0.2469775474956822,\n",
       " 0.24179620034542315,\n",
       " 0.24006908462867013,\n",
       " 0.24006908462867013,\n",
       " 0.23661485319516407,\n",
       " 0.231433506044905,\n",
       " 0.229706390328152,\n",
       " 0.22452504317789293,\n",
       " 0.21761658031088082,\n",
       " 0.2158894645941278,\n",
       " 0.2158894645941278,\n",
       " 0.2141623488773748,\n",
       " 0.2141623488773748,\n",
       " 0.21070811744386875,\n",
       " 0.20725388601036268,\n",
       " 0.20379965457685664,\n",
       " 0.20379965457685664]"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = []\n",
    "for i in np.arange(0,10,0.1):\n",
    "    smooth_lpr_s1, smooth_lpr_s0, smooth_lpr_w1s1, smooth_lpr_w1s0 = fitting(i)\n",
    "    accuracy.append(prediction())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3845184",
   "metadata": {},
   "source": [
    "5. (2pt) Plot how accuracy depends on α. Use log-scale for α!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "5fdaf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb8e7070280>]"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b422d",
   "metadata": {},
   "source": [
    "## (20pt) Interpretation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e266d",
   "metadata": {},
   "source": [
    "Naive Bayes is interpretable in a little similar fashion like linear regression. But in only a little similar fashion. Namely, we can find the words that are the best predictors that an email is spam, and the best predictors that email is non-spam. And we want to look at reasonably common words only, say more frequent than 10 times in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98211ca3",
   "metadata": {},
   "source": [
    "1. (10pt) Which words are the best predictors that an email is spam? These are the word where Pr(S = 1|W = 1) is large and Pr(S = 0|W = 1) is small, or to put it differently, where log Pr(S = 1|W = 1)−logPr(S = 0|W = 1) is large. Explain why this is the case. Hint: you may re-check the concept of log-likelihood and how that is used for prediction. Hint 2: you may imagine you receive 60k 1-word emails (one for each word in your vocabulary). Which ones are most likely spam, and which ones are least likely spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "e144c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_spam_index = np.where((smooth_lpr_w1s1 - smooth_lpr_w1s0 > 6) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "31b40757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121,)"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_spam_index[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "46ae8178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  808,   837,  1086,  2322,  6444,  7340,  8070,  8861,  8938,\n",
       "        9481,  9852, 10372, 11052, 11423, 11647, 12714, 13248, 14200,\n",
       "       14234, 14236, 14814, 15016, 15149, 16107, 16281, 16350, 17023,\n",
       "       17242, 17904, 19271, 19278, 19573, 19771, 19772, 20491, 20511,\n",
       "       21389, 21435, 21877, 22288, 22391, 22479, 23020, 23353, 24420,\n",
       "       25149, 25964, 27568, 27910, 27972, 29339, 29663, 30113, 30163,\n",
       "       30184, 30187, 31413, 31442, 35010, 35564, 36056, 36562, 36957,\n",
       "       37396, 37755, 38273, 38470, 38694, 41555, 41565, 41612, 41624,\n",
       "       42269, 42661, 43853, 44382, 44653, 44751, 44811, 44907, 46144,\n",
       "       46240, 46353, 46620, 46747, 46875, 47131, 47134, 47310, 47332,\n",
       "       47706, 47825, 47880, 47961, 49020, 49500, 49649, 49753, 50001,\n",
       "       51321, 51565, 51659, 51811, 51858, 52068, 52244, 52442, 53256,\n",
       "       54171, 54507, 55397, 56722, 56723, 57094, 57710, 58740, 58875,\n",
       "       59123, 59592, 59983, 60255])"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_spam_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "08e2ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_spam_words = []\n",
    "for i in range(len(vocabulary)):\n",
    "    if i in high_spam_index[1]:\n",
    "        high_spam_words.append(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "a8cea32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1302',\n",
       " '1341',\n",
       " '1618',\n",
       " '3005',\n",
       " 'advertiser',\n",
       " 'amazed',\n",
       " 'anytime',\n",
       " 'assets',\n",
       " 'astonishment',\n",
       " 'awesome',\n",
       " 'bankruptcies',\n",
       " 'believer',\n",
       " 'biz',\n",
       " 'bonus',\n",
       " 'boyfriend',\n",
       " 'capitalfm',\n",
       " 'celebrity',\n",
       " 'classifieds',\n",
       " 'cleaned',\n",
       " 'cleanest',\n",
       " 'commercialemail',\n",
       " 'complies',\n",
       " 'concealed',\n",
       " 'costing',\n",
       " 'crammed',\n",
       " 'creditors',\n",
       " 'dare',\n",
       " 'debts',\n",
       " 'desirous',\n",
       " 'downline',\n",
       " 'downpayment',\n",
       " 'duplicates',\n",
       " 'earning',\n",
       " 'earnings',\n",
       " 'emailamendtext',\n",
       " 'embark',\n",
       " 'esq',\n",
       " 'estate',\n",
       " 'excited',\n",
       " 'fabulous',\n",
       " 'fairchild',\n",
       " 'fantastic',\n",
       " 'filtered',\n",
       " 'fm',\n",
       " 'gambling',\n",
       " 'girlfriend',\n",
       " 'grumbled',\n",
       " 'hobby',\n",
       " 'hottest',\n",
       " 'hrs',\n",
       " 'inflation',\n",
       " 'instant',\n",
       " 'intrusion',\n",
       " 'invested',\n",
       " 'investing',\n",
       " 'investments',\n",
       " 'jumped',\n",
       " 'juno',\n",
       " 'lottery',\n",
       " 'mailers',\n",
       " 'marketer',\n",
       " 'mci',\n",
       " 'merciless',\n",
       " 'millionaires',\n",
       " 'mlm',\n",
       " 'mortgage',\n",
       " 'msn',\n",
       " 'murkowski',\n",
       " 'overflowing',\n",
       " 'overloaded',\n",
       " 'owed',\n",
       " 'owning',\n",
       " 'paste',\n",
       " 'percentages',\n",
       " 'porn',\n",
       " 'premium',\n",
       " 'privacy',\n",
       " 'prodigy',\n",
       " 'profits',\n",
       " 'promotional',\n",
       " 'reap',\n",
       " 'recieve',\n",
       " 'recruiting',\n",
       " 'regs',\n",
       " 'relax',\n",
       " 'removes',\n",
       " 'resell',\n",
       " 'reselling',\n",
       " 'retail',\n",
       " 'retire',\n",
       " 'rip',\n",
       " 'robbery',\n",
       " 'rockland',\n",
       " 'rolling',\n",
       " 'scam',\n",
       " 'secrets',\n",
       " 'sellers',\n",
       " 'sends',\n",
       " 'sexually',\n",
       " 'someday',\n",
       " 'spam',\n",
       " 'specials',\n",
       " 'spokane',\n",
       " 'spouting',\n",
       " 'staggering',\n",
       " 'stealth',\n",
       " 'stocks',\n",
       " 'surf',\n",
       " 'teen',\n",
       " 'testimonial',\n",
       " 'totals',\n",
       " 'undeliverable',\n",
       " 'undeliverables',\n",
       " 'unproductive',\n",
       " 'vanish',\n",
       " 'vulgarity',\n",
       " 'wall',\n",
       " 'webmaster',\n",
       " 'win95',\n",
       " 'wrapping',\n",
       " 'yahoo']"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_spam_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ede85",
   "metadata": {},
   "source": [
    "Since we are calculating log-likehood of an email being spam with a certain and log-likelihood of an email not being spam with that word, we can compare the two likelihoods and by using the difference in likelihood, we can find the words where the diffrence is greater therefore emails with those words are more likely to be spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7272d",
   "metadata": {},
   "source": [
    "2. (10pt) Find 10 best words to predict spam and 10 best words to predict non-spam. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "794e6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_spam_index = np.where((smooth_lpr_w1s1 - smooth_lpr_w1s0 > 6.92) == True)\n",
    "high_spam_index[1].shape\n",
    "high_spam_words = []\n",
    "for i in range(len(vocabulary)):\n",
    "    if i in high_spam_index[1]:\n",
    "        high_spam_words.append(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "f887822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bonus',\n",
       " 'debts',\n",
       " 'earning',\n",
       " 'fantastic',\n",
       " 'hottest',\n",
       " 'mlm',\n",
       " 'profits',\n",
       " 'relax',\n",
       " 'resell',\n",
       " 'secrets',\n",
       " 'spam',\n",
       " 'stealth']"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_spam_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10457b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
